# Significance, Probability and the Normal Distribution

## Significance and Probability

Hypothesis testing fundamentally builds on probabilities - or more precisely probabilities of error which is an estimation for the likelihood of the H~0~ being true given the data. In other words, we are interested in how likely the null hypothesis (H~0~) is. If the null hypothesis (H~0~) is very unlikely, for instance lower than 5 percent, we can assume that the null hypothesis is not true and thus reject it and instead assume that our test hypothesis (H~1~) is correct.
 

This type of probability, the probability ( the probability that the null hypothesis is true) is typically provided in the form of p-values. In a more prosaic (and also coarse-grained, imprecise manner), p-values are an estimate of how likely an outcome is a result of chance. We will delve a little deeper into probabilities and how they relate to hypothesis testing below.

### Significance Levels{-}

Before conducting a study, it is advisable to determine the so-called significance level or $\alpha$ level. This $\alpha$ level or level of significance indicates how high the p-value can be without having to assume that there is a significant relationship between the variables. It is customary to differentiate between three levels of significance (also called $\alpha$ levels):

* p < .001: *highly significant*  - indicated by three stars (***)

* p < .01: *very significant* - indicated by two stars (**)

* p < .05: *significant* - indicated by one star (*)

Variables with a p value that is smaller than .1 but larger than .05 are sometimes referred to as being *marginally significant* (+).  

Variables that are not significant are commonly referred to or labeled as *n.s.* (not significant). As we stated above, before we perform a test, we determine a value above which we reject the null hypothesis, the so-called significance level. It's usually 5%. If the error probability is less than 5% (p <. 05), we reject the null hypothesis. Conclusion: The relationship between the variables is statistically significant. It is important to note here that the H~1~ (or Test Hypothesis) is correct only because the null hypothesis can be rejected! Statistics can NEVER prove hypotheses but only reject Null Hypotheses which leads us to accept the H~1~ as preliminary accepted or not-yet-rejected. So all knowledge is preliminary in the empirical sciences.

### Probability{-}

In the following, we will turn to probability and try to understand why probability is relevant for testing hypotheses. This is important at this point because statistics, and thus hypothesis testing, fundamentally builds upon probabilities and probability distributions. In order to understand how probability works, we will investigate what happens when we flip a coin. The first question that we will be addressing is "*What is the probability of getting three Heads when flipping a coin three times?*".

The probability of getting three heads when flipping a coin three times is .5 to the power of 3: .5^3^ = .5 times .5 times .5 = .125. The probability of getting Heads twice when flipping the coin three times is .375. How do we know?


The probability of getting 3 heads in tree tosses is 12.5 percent:

.5^3^ = .5 * .5 * .5 = .125

The probability of getting 2 heads in tree tosses is 37.5 percent:

.125 + .125 + .125 = 0.375

But how do we know this? Well, have  look at the table below.

***

```{r eval=T, echo=F, message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(flextable)
numbers <- matrix(c("Head" , "Head" , "Head" , "3" , "0" , "0.125", "Head" , "Head" , "Tails" , "2" , "1" , "0.125", "Head" , "Tails" , "Head" , "2" , "1" , "0.125", "Tails" , "Head" , "Head" , "2" , "1" , "0.125", "Head" , "Tails" , "Tails" , "1" , "2" , "0.125", "Tails" , "Head" , "Tails" , "1" , "2" , "0.125", "Tails" , "Tails" , "Head" , "1" , "2" , "0.125", "Tails" , "Tails" , "Tails" , "0" , "3" , "0.125"), byrow = T, nrow = 8)
colnames(numbers) <- c("Toss 1","Toss 2","Toss 3","Heads", "Tails", "Probabilty")
numbers %>%
  as.data.frame() %>%
  flextable() %>%
  flextable::set_table_properties(width = .75, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "All possible results of 3 coin tosses.")  %>%
  flextable::border_outer()
```

 ***

Given this table, we are in fact, in a position to calculate the probability of getting 100 heads in 100 coin tosses because we can simply fill in the numbers in the formulas used above:  .5^100^ = 7.888609 * 10^-31^

Okay, let us make a bet..

* If head shows, I win a dollar.

* If tails shows, you win a dollar.

But given that you know I am cheeky bastard, you do not trust me and claim that I will cheat. But how will you know that I cheat? At which point can you claim that the result is so unlikely that you are (scientifically backed) allowed to claim that I cheat and have manipulated the coin?



```{r normal2, echo=F, message=FALSE, warning=FALSE}
Type <- c(rep("NormalCoin", 500000), 
            rep("ManipulatedCoin", 500000))
Tosser <- rep(paste("Tosser", 1:500000, sep = ""), 2) 
Frequency <- c(rnorm(500000, 50, 3),
               rnorm(500000, 53, 3))
particletable <- data.frame(Type, Tosser, Frequency)
library(ggplot2)
ggplot(particletable,aes(x=Frequency, fill = Type))+
  geom_histogram(aes(y=..density..), alpha=.5, position = "identity")+
  theme_bw()+
  labs(y = "Density", x = "Distribution of <heads> in coin tosses with a normal coin and a manipulated coin.") + 
  scale_fill_manual(values = c('lightgreen', 'orange')) 
```


So before we actually start with the coin tossing, you  operationalize your hypothesis: 

* H~0~: The author (Martin) is not cheating (heads shows just as often as tails).

* H~1~: The author (Martin) is cheating (heads shows so often that the probability of the author not cheating is lower than 5 percent)

We now toss the coin and head shows twice. The question now is whether head showing twice is lower than 5 percent. 

We toss the coin 3 times. Head shows twice. How likely is it that I do not cheat and head falls more than twice anyway? (In other words, what is the probability p that I win twice or more and not cheat?) If you set the significance level at .05, could you then accuse me of being a cheater? 

As you can see in the fourth column, there are three options that lead to heads showing twice (rows 2, 3, and 4). If we add these up (0.125 + 0.125 + 0.125 = 0.375). Also, we need to add the case where head shows 3 times which is another .125 (0.375 + 0.125 = .5), then we find out that the probability of heads showing at least twice in three coin tosses is 50 percent and thus 10 times more than the 5-percent threshold that we set initially. Therefore, you cannot claim that I cheated. 

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
probdata1 <- matrix(c(0.125, 0.375, 0.375, 0.125), byrow = T, nrow = 1)
colnames(probdata1) <- c("0 Heads", "1 Head", "2 Heads", "3 Heads")
# inspect data
probdata1 %>%
  as.data.frame() %>%
  flextable() %>%
  flextable::set_table_properties(width = .75, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Probability of successes in coin tosses.")  %>%
  flextable::border_outer()
```

```{r probcoin01, echo=F, eval = T, message=FALSE, warning=FALSE}
Probability <- c(0.125, 0.375, 0.375, 0.125)
Heads <- 0:3
probs <- data.frame(Probability, Heads) %>%
  dplyr::mutate(Heads = factor(Heads))
probs %>%
  ggplot(aes(x = Heads, y = Probability)) +
  geom_bar(stat = "identity") +
  geom_text(label = Probability, nudge_y = +.05) +
  coord_cartesian(ylim = c(0, 0.5)) +
  labs(title = "Probabilities of having 0, 1, 2, or 3 times head in 3 coin tosses.") +
  theme_bw()
```

We can do the same and check the probabilities of having heads in 10 coin tosses - if we plot the resulting probabilities, we get the bar plot shown below.

```{r probcoin02, echo=F, eval = T, message=FALSE, warning=FALSE}
Probability <- round(dbinom(0:10, 10, 0.5), 3)
Heads <- 0:10
probs <- data.frame(Probability, Heads) %>%
  dplyr::mutate(Heads = factor(Heads))
probs %>%
  ggplot(aes(x = Heads, y = Probability)) +
  geom_bar(stat = "identity") +
  geom_text(label = Probability, nudge_y = +.05) +
  coord_cartesian(ylim = c(0, 0.4)) +
  labs(title = "Probabilities of having head in 10 coin tosses.") +
  theme_bw()
```

The distribution looks more bell-shaped with very low probabilities of getting 0 or 1 as well as 9 or 10 times heads in the 10 coin tosses and a maximum at 5 times heads in 10 coin tosses. In fact, the probability of having 0 and 10 times head is .001 (or 1 in a 1000 attempts - one attempt is tossing a coin 10 times) - the probability of having heads once and 9 times is.01 or 1 in 100 attempts. In contrast, the probability of having 5 times head is .246 or about 25% (1 in 4 attempts will have 5 heads). Also, we can sum up probabilities: the probability of getting 8 or more heads in 10 coin tosses is the probability of getting 8, 9, and 10 times heads (.044 + .010 + 0.001 = .055 = 5.5 percent).

Calculating the probabilities for three or even 10 coin tosses is still manageable manually but is there an easier way to calculate probabilities? A handier way is have a computer calculate probabilities and the code below shows how to do that in R - a very powerful and flexible programming environment that has been designed for quantitative analysis (but R can, in fact, do much more - this website, for instance, is programmed in R).

The code chunk below calculates the probabilities of having 0, 1, 2, and 3 times head in 3 tosses.

```{r message=FALSE, warning=FALSE}
# probabilities of  0, 1, 2 and 3 times head in 3 coin tosses
dbinom(0:3, 3, 0.5)
```

The code chunk below calculates the probabilities of having 2 or 3 times head in 3 tosses.

```{r message=FALSE, warning=FALSE}
# probabilities of  2 or 3 times head in 3 coin tosses
sum(dbinom(2:3, 3, 0.5))
```

The code chunk below calculates the probabilities of having 100 head in 100 tosses.


```{r message=FALSE, warning=FALSE}
# probability of  100 times head in 100 coin tosses
dbinom(100, 100, 0.5)
```

The code chunk below calculates the probabilities of having 58 or more times head in 100 tosses.

```{r message=FALSE, warning=FALSE}
# probability of  58 to a 100 times head in 100 coin tosses
sum(dbinom(58:100, 100, 0.5))
```

The code chunk below calculates the probabilities of having 59 or more times head in 100 tosses.


```{r message=FALSE, warning=FALSE}
# probability of  59 to a 100 times head in 100 coin tosses
sum(dbinom(59:100, 100, 0.5))
```

The code chunk below calculates the number of heads in 100 tosses where the probability of getting that number of heads or more sums up to 5 percent (0.05).


```{r message=FALSE, warning=FALSE}
# at which point does the probability of getting head 
# dip below 5 percent in 100 coin tosses?
qbinom(0.05, 100, 0.5, lower.tail=FALSE)
```

```{r echo=F, eval = F, message=FALSE, warning=FALSE}
# at which point does the probability of getting head 
# dip below 5 percent in 100 coin tosses?
qnorm(0.05, lower.tail=TRUE)
```

Let's go back to our example scenario. In our example scenario, we are dealing with a directed hypothesis and not with an un-directed/non-directed hypothesis because we claimed in our H~1~ that I was cheating and would get *more* heads than would be expected by chance ($\mu_{Martin}$ $>$ $\mu_{NormalCoin}$). For this reason, the test we use is one-tailed. When dealing with un-directed hypotheses, you simply claim that the outcome is either higher or lower - in other words the test is two-tailed as you do not know in which direction the effect will manifest itself. 

To understand this a more thoroughly, we will consider tossing a coin not merely 3 but 100 times. The Figure below shows the probabilities for the number of heads showing when we toss a coin 100 from 0 occurrences to 100 occurrences.

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
# load packages
library(tidyverse)
# set up data
p100 <- dbinom(0:100, 100, 0.5)
w100 <- c(0:100)
df <- data.frame(w100, p100)
ggplot(df, aes(x = w100, y = p100)) +
  geom_bar(stat="identity", fill = "gray") +
  labs(x = "Frequency Count (Head)", y = "Probability") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank())
```

The next figure shows at which number of heads the cumulative probabilities dip below 5 percent for two-tailed hypotheses. According to the graph, if head shows up to 40 or more often than 60 times, the cumulative probability dips below 5 percent. Applied to our initial bet, you could thus claim that I cheated if head shows less than 41 times or more than 60 times (if out hypothesis were two-tailed - which it is not). 

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
df <- df %>%
  dplyr::mutate(cump = cumsum(p100),
                col = ifelse(cump <= 0.025 | cump >= 0.975, T, F))
ggplot(df, aes(x = w100, y = p100, fill = col)) +
  geom_bar(stat="identity") +
  labs(x = "Frequency Count (Head)", y = "Probability") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  scale_fill_manual(values = c("gray", "red")) +
  annotate(geom = "text", label = expression(paste(mu^1 !=  mu^2, sep = "")), x = 70, y = .05)
```

The Figure below shows at which point the probability of heads showing dips below 5 percent for one-tailed hypotheses. Thus, according to the figure below, if we toss a coin 100 times and head shows 59 or more often, then you are justified in claiming that I cheated.

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
df <- df %>%
  dplyr::mutate(cump = cumsum(p100),
                col = ifelse(cump >= 0.95, T, F))
ggplot(df, aes(x = w100, y = p100, fill = col)) +
  geom_bar(stat="identity") +
  labs(x = "Frequency Count (Head)", y = "Probability") +
  theme_bw() +
  theme(panel.grid.major = element_blank(), 
        panel.grid.minor = element_blank(),
        legend.position = "none") +
  scale_fill_manual(values = c("gray", "red")) +
  annotate(geom = "text", label = expression(paste(mu^1 >=  mu^2, sep = "")), x = 70, y = .05)
```


When comparing the two figures above, it is notable that the number at which you can claim I cheated differs according to whether the H~1~ as one- or two-tailed. When formulating a one-tailed hypothesis, then the number is lower compared with the the number at which you can reject the H~0~ if your H~1~ is two-tailed. This is actually the reason for why it is  preferable to formulate more precise, one-tailed hypotheses (because then, it is easier for the data to be sufficient to reject the H~0~). 


### The Normal Distribution{-}

It is important to note here that the above described calculation of probabilities does not work for numeric variables that are interval-scaled. The reason for this is that it is not possible to calculate the probabilities for all possible outcomes of a reaction time experiment (where time is a continuous variable). In such cases, we rely on *distributions* in order to determine how likely or probable a certain outcome is. 

When relying on distributions, we determine whether a certain value falls within or outside of the area of a distribution that accounts for 5 percent of the entire area of the distribution - if it falls within the area that accounts for less than  5 percent of the total area, then the result is called *statistically significant* (see the normal distribution below) because the outcome is very unlikely.

The normal distribution (or Gaussian curve or Gaussian distribution) shown in the figure above has certain characteristics that can be derived mathematically. Some of these characteristics relate to the area of certain sections of that distribution. More generally, the normal distribution is a symmetric, bell-shaped probability distribution, used as the theoretical model for the distribution of physical and psychological variables. In fact, many variables in the real world are normally distributed: shoe sizes, IQs, sleep lengths, and many , many more.

The normal distribution is very important because it underlies many statistical procedures in one form or another. The normal distribution is a symmetrical, continuous distribution where  

* the arithmetic mean, the median, and the mode are identical and have a value of 0 (and are thus identical);  
* 50% of the area under the bell-shaped curve are smaller than the mean;  
* 50% of the area under the bell-shaped curve are bigger than the mean;  
* 50% of the area under the bell-shaped curve are within -0.675 and +0.675 standard deviations from the mean (0);  
* 95% of the area under the bell-shaped curve are within -1.96 and +1.96 standard deviations from the mean (0);  
* 99% of the area under the bell-shaped curve are within -2.576 and +2.576 standard deviations from the mean (0).


There is also a very interesting aspect to the normal distribution that relates to the means (averages) of samples drawn from any type of distribution: no matter what type of distribution we draw samples from, the distribution of the means will approximate a normal distribution. In other words, if we draw samples form a very weird looking distribution, the means of these samples will approximate a normal distribution. This fact is called the *Central Limit Theorem*. What makes the central limit theorem so  remarkable is that this result holds no matter what shape the original population  distribution may have been.

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
# load packages
library(grDevices)
library(graphics)
### normal distribution: Mu=0, Sigma=1: Standard normal
library(graphics)
plot(dnorm, -4, 4, axes = F, xlab = "Standard Deviations (z-score)", ylab = "Probability")
axis(1, at = seq(-4, 4, 1), labels = seq(-4, 4, 1))
axis(2, at= c(0.0, 0.2, 0.4), labels= c("0.0", "0.2", "0.4"))
text(3, 0.2, expression(paste(mu, "=0, ", sigma^2, "=1", sep = "")))
box()
lines(c(0, 0), c(-1, 0.5), col = "lightgrey")
lines(c(-0.675, 0.675), c(rep(0.3, 2)), col="red", lty=2)
text(0, 0.35, "50.0%")
lines(c(-1.96,1.96), c(rep(0.2, 2)), col="blue", lty=2)
text(0, 0.25, "95%")
lines(c(-2.576,2.576), c(rep(0.1, 2)), col="green", lty=2)
text(0, 0.15, "99%")
lines(c(-0.675, -0.675), c(-1, 0.5), col = "red")
lines(c(0.675, 0.675), c(-1, 0.5), col = "red")
lines(c(-1.96, -1.96), c(-1, 0.5), col = "blue")
lines(c(1.96, 1.96), c(-1, 0.5), col = "blue")
lines(c(-2.576, -2.576), c(-1, 0.5), col = "green")
lines(c(2.576, 2.576), c(-1, 0.5), col = "green")
text(-2.9, 0.025, "-2.576", cex = .75)
text(-1.6, 0.025, "-1.96", cex = .75)
text(-1, 0.025, "-0.675", cex = .75)
text(2.9, 0.025, "2.576", cex = .75)
text(1.7, 0.025, "1.96", cex = .75)
text(1, 0.025, "0.675", cex = .75)
```

As shown above, 50 percent of the total area under the curve are to left and 50 percent of the right of the mean value. Furthermore, 68 percent of the area are within -1 and +1 standard deviations from the mean; 95 percent of the area lie between -2 and +2 standard deviations from the mean;  99.7 percent of the area lie between -3 and +3 standard deviations from the mean. Also, 5 percent of the area lie outside -1.96 and +1.96 standard deviations from the mean (if these areas are combined) (see the Figure below). This is important. because we can reject null hypotheses if the probability of them being true is lower than 5 percent. This means that we can use the normal distribution to reject null hypotheses if our dependent variable is normally distributed. 


```{r echo=F, eval = T, message=FALSE, warning=FALSE}
plot(dnorm, -4, 4, axes = F, xlab = "Standard Deviations (z-score)", ylab = "Probability")
axis(1, at = seq(-4, 4, 1), labels = seq(-4, 4, 1))
axis(2, at= c(0.0, 0.2, 0.4), labels= c("0.0", "0.2", "0.4"))
text(3, 0.2, expression(paste(mu, "=0, ", sigma^2, "=1", sep = "")))
box()
lines(c(1.96, 1.96), c(0, 0.5), col="lightgrey", lty=2)
text(3, 0.15, "2.5%")
text(3, 0.085, "(sd = 1.96 (z-score))")
lines(c(-1.96, -1.96), c(0, 0.5), col="lightgrey", lty=2)
text(-3, 0.15, "2.5%")
text(-3, 0.085, "(sd = -1.96 (z-score))")
```

Finally, 5 percent of the area lies beyond +1.68 standard deviations from the mean (see the Figure below).    

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
plot(dnorm, -4, 4, axes = F, xlab = "Standard Deviations (z-score)", ylab = "Probability")
axis(1, at = seq(-4, 4, 1), labels = seq(-4, 4, 1))
axis(2, at= c(0.0, 0.2, 0.4), labels= c("0.0", "0.2", "0.4"))
text(3, 0.2, expression(paste(mu, "=0, ", sigma^2, "=1", sep = "")))
box()
lines(c(1.64, 1.64), c(0, 0.5), col="lightgrey", lty=2)
text(3, 0.15, "5% (sd = 1.64 (z-score))")
```

These properties are extremely useful when determining the likelihood of values or outcomes that reflect certain interval-scaled variables. 

***

<div class="warning" style='padding:0.1em; background-color:#51247a; color:#f2f2f2'>
<span>
<p style='margin-top:1em; text-align:center'>
<b>EXERCISE TIME!</b></p>
<p style='margin-left:1em;'>
</p></span>
</div>

<div class="question">` 

1. Create a table with the possible outcomes and probabilities of 4 coin tosses (you can consider the table showing the outcomes of three coin tosses above as a guideline).

<details>
  <summary>Answer</summary>
  ```{r message=FALSE, warning=FALSE}
  sum(dbinom(4, 7, 0.5))
  ```
</details>

2. How likely is it for heads to show exactly 3 times when tossing a coin 7 times?

<details>
  <summary>Answer</summary>
  ```{r message=FALSE, warning=FALSE}
  sum(dbinom(3, 7, 0.5))
  ```
</details>

3. How likely is it for heads to show exactly 2 or 5 times when tossing a coin 7 times?

<details>
  <summary>Answer</summary>
  ```{r message=FALSE, warning=FALSE}
  sum(dbinom(c(2, 5), 7, 0.5))
  ```
</details>

4. How likely is it for heads to show 5 or more times when tossing a coin 7 times?

<details>
  <summary>Answer</summary>
  ```{r message=FALSE, warning=FALSE}
  sum(dbinom(5:7, 7, 0.5))
  ```
</details>

5. How likely is it for heads to show between 3 and 6 times when tossing a coin 7 times?

<details>
  <summary>Answer</summary>
  ```{r message=FALSE, warning=FALSE}
  sum(dbinom(3:6, 7, 0.5))
  ```
</details>

</div>`

***


### Non-normality: skewness and kurtosis{-}

Depending on the phenomenon you are investigating, the distribution of that phenomenon can be distributed non-normally. Two factors causing distributions to differ from the normal distribution (that is two factors causing distributions to be non-normal) are *skewness* and *kurtosis*.

#### Skewness{-}

Skewed distributions are asymmetrical and they can be positively or negatively skewed. The tail of a *negatively skewed distribution* points towards negative values (to the left) which means that the distribution leans towards the right (towards positive values) while the tail of a *positively skewed distribution* points towards positive values (to the right) which means that the distribution leans towards the left (towards negative values). 

Another characteristic of skewed distributions is that the mean and median of a distribution differ. If the median is greater than the mean, the distribution is negatively skewed (the long tail points towards negative values). If the median is smaller than the mean, the distribution is positively skewed (the long tail points towards positive values).

Negatively skewed

* Tail points towards negative values (to the left)
* Median is greater than the mean

Positively skewed

* Tail points towards positive values (to the right)
* Median is lower than the mean

```{r skewness, echo = F, warning=F, message=F}
skew <- data.frame(rbeta(1000000,10,2)) %>%
  mutate(pskew = rbeta(1000000,2,10),
         nrm = rbeta(1000000,10,10)) %>%
  dplyr::rename(nskew = colnames(.)[1],
                pskew = colnames(.)[2],
                nrm = colnames(.)[3])
# inspect
ggplot(skew, aes(x = nskew, alpha = .5)) +
  geom_density(fill = "lightgreen", alpha = .5, color = "lightgreen") +
  geom_density(aes(x = pskew), fill = "orange", alpha = .5, color = "orange") +
  geom_density(aes(x = nrm), fill = "lightgray", alpha = .5, color = "lightgray") +
  theme_bw() +
  theme(legend.position = "none") +
  labs(title = "Positively skewed distribution (orange), \nnormal, non-skewed distribution (lightgray), \nnegatively skewed distribution (lightgreen)", x = "", y = "Density")
```

To show how we can calculate skewness (or if a distribution is skewed), we generate a sample of values.


```{r calskew1, warning=F, message=F}
SampleValues <- sample(1:100, 50)
# inspect data
summary(SampleValues)
```

We apply the `skewness` function from the `e1071` package to the sample scores to calculated skewness. The `skewness` function  allows to calculate skewness in three different ways:

Type 1 (This is the typical definition used in many older textbooks): 
\begin{equation}
g_1 = m_3 / m_2^{(3/2)}.
\end{equation}

Type 2 (Used in SAS and SPSS): 
\begin{equation}
G_1 = g_1 * sqrt(n(n-1)) / (n-2)
\end{equation}

Type 3 (Used in MINITAB and BMDP): 
\begin{equation}
b_1 = m_3 / s^3 = g_1 ((n-1)/n)^{(3/2)}
\end{equation}

All three formulas have in common that the more negative values are, the more strongly positively skewed are the data (distribution leans to the left) and the  more positive the values are, the more strongly negatively skewed are the data (distribution leans to the right). Here we use the second formula (by setting `type = 2`) that is also used in SPSS.


```{r calskew2, warning=F, message=F}
library(e1071)
skewness(SampleValues, type = 2)
```

If the reported skewness value is negative, then  the distribution is positively skewed. If the value is positive, then the distribution is negatively skewed. If the value is lower than -1 or greater than +1, then the distribution can be assumed to be substantively skewed [@hair2017dist]. 

#### Kurtosis{-}

Another way in which distributions can differ from the normal distribution  relates to the thickness of the tails and the spikiness of the distribution. If distributions are bell-shaped like the normal distribution, they are called *mesokurtic*. If distributions are symmetrical but they are more spiky than the normal distribution, they are called *leptokurtic*. If symmetrical distributions are flatter and have bigger tails than the normal distribution, the distributions are called *platykurtic*.  

```{r kurtosis, echo = F, warning=F, message=F}
kur <- data.frame(x <- seq(-4, 4, length.out = 100)) %>%
  mutate(lep = dt(x, 1)^2*5,
         nrm = dnorm(x),
         thk = dt(x, 1)) %>%
  dplyr::rename(x = colnames(.)[1],
                lepto = colnames(.)[2],
                normal = colnames(.)[3],
                platy = colnames(.)[4])
ggplot(kur, aes(x, lepto)) + 
#  geom_line(color = "orange", size = 1.5, linetype = 2, alpha = .5) +
  geom_ribbon(aes(x = x, ymax = lepto), ymin=0, alpha=0.5, fill = "orange") +
  geom_ribbon(aes(x = x, ymax = normal), ymin=0, alpha=0.5, fill = "lightgray") +
  geom_ribbon(aes(x = x, ymax = platy), ymin=0, alpha=0.5, fill = "lightgreen") +
  theme_bw() + 
  labs(title = "Leptokurtic distribution (orange, spicky)\nMesokurtic distribution (gray, normal)\nPlatykurtic distribution (green, flattened)", x = "", y = "") +
  theme(axis.text.x = element_blank(),
        axis.text.y = element_blank(),
        axis.ticks = element_blank()) +
  coord_cartesian(xlim = c(-4,4), ylim = c(0,.5))
```

To show how we can calculate kurtosis (or if a distribution suffers from kurtosis), we apply the `kurtosis` function from the `e1071` package to the sample scores we generated above /when we calculated skewness).


```{r calclurt03, warning=F, message=F}
kurtosis(SampleValues)
```

As the kurtosis value is positive, the distribution is leptokurtic (if it were negative, the distribution would be platykurtic). As a rule of thumb, values greater than +1 indicate that the distribution is too peaked while values lower than â€“1 indicate that the distribution is substantively platykurtic [@hair2017dist, pp61].

The kurtosis score can thus be interpreted as follows:

* A values of 0 means that the distribution is perfectly *mesokurtic* with Values between -0.5 and 0.5 suggesting that the distribution is *approximately mesokurtic*

* Values between -0.5 and -1 mean that the distribution is *moderately platykurtic* with values smaller than -1 indicating that the distribution is *platykurtic*.

* Values between 0.5 and 1 mean that the distribution is *moderately leptokurtic* with values greater than 1 indicating that the distribution is *leptokurtic*


### The Binomial Distribution{-}

A distribution which is very similar to the normal distribution is the "binomial distribution". The binomial distribution displays the probability of binary outcomes. So, in fact, the distribution of the coin flips above represents a binomial distribution. However, the binomial distribution cane be (as above in case of the coin flips) but does not have to be symmetric (like the normal distribution). 

To illustrate this, let us consider the following example: we are interested in how many people use discourse *like* in Australian English. We have analyzed a corpus, for instance the Australian component of the [International Corpus of English](https://www.ice-corpora.uzh.ch/en.html) and found that 10 percent of all speakers have used discourse *like*. In this example, the corpus encompasses 100 speakers (actually the the spoken section of the Australian component of the ICE represents more than 100 speakers but it will make things easier for us in this example).

Now, given the result of our corpus analysis, we would like to know what percentage of speakers of Australian English use discourse *like* with a confidence of 95 percent. Given the corpus study results, we can plot the expected binomial distribution of discourse *like* users in Australian English.

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
# load packages
library(grDevices)
# set up data
p50 <- dbinom(0:50, 50, 0.1)
w50 <- c(0:50)
wtb <- rbind(w50, p50)
colnames(wtb) <- c(0:50)
rownames(wtb) <- c("Likeuser", "prob")
xseq <- seq(0.7, 60.7, 1.2)
xseq <- xseq[c(1, 20, 40, 60, 80, 100)]
x <- barplot(wtb[2,], 
             ylim = c(0, 0.2), 
             ylab = "Probability", 
             xlab = "Frequency Count (discourse like users)", 
             axes = F, 
             axisnames = F, 
             col = "lightgrey")
axis(1, at= seq(0.7, 60.7, (60.7-0.7)/10), labels= seq(0, 50, 5))
axis(2, at= seq(0.0, 0.2, 0.05), labels= seq(0.0, 0.2, 0.05), cex = .75)
box()
```


The bar graph above shows the probability distribution of discourse *like* users in Australian English. The first thing we notice is that the binomial distribution is not symmetric as it is slightly left-skewed. If we would increase the number of draws or speakers in our example, the distribution would, however, approximate the normal distribution very quickly because the binomial distribution approximates the normal distribution for large N or if the probability is close to .5. Thus, it is common practice to use the normal distribution instead of the binomial distribution. 

The bar graph below also shows the probability distribution of discourse *like* users in Australian English but, in addition, it is color-coded: bars within the 95 percent confidence interval are *lightgray*, the bars in red are outside the 95 percent confidence interval. In other words, if we repeated the corpus analysis, for instance 1000 times, on corpora which represent speakers from the same population, then 95 percent of samples would have between 1 and 8 discourse *like* users. 

```{r echo=F, eval = T, message=FALSE, warning=FALSE}
# sum probabilities
sp <- cumsum(wtb[2,])
ttsp <- as.vector(unlist(sapply(sp, function(x){
  x <- ifelse(x <= 0.025, TRUE,
    ifelse(x >= 0.975, TRUE, FALSE))
    })))
barplot(wtb[2,], 
        ylim = c(0, 0.2), 
        ylab = "Probability", 
        xlab = "Frequency Count (discourse like users)",
        axes = F, 
        axisnames = F, 
        col = ifelse(ttsp == T, "red", "lightgrey"))
axis(1, at= seq(0.7, 60.7, (60.7-0.7)/10), labels= seq(0, 50, 5))
axis(2, at= seq(0.0, 0.2, 0.05), labels= seq(0.0, 0.2, 0.05))
box()
#grid()
text(75, 0.06, expression(paste(mu^1 !=  mu^2, sep = "")))
```

In addition to the normal distribution and the binomial distribution, there are many other distributions which underlay common procedures in quantitative analyses. For instance, the t- and the F-distribution or the Chi-distribution. However, we will not deal with these distributions here.


## z-scores

We have encountered z-scores before - the values on the x-axis in the figures showing the normal distributions are, in fact, z-scores. This means that z-scores reflect how many standard deviations a score is distant from the mean of a distribution. But why are these z-scores useful?

Imagine you want to compare the performance of two students that have written tests in different classes. Student A had a score of 50 out of 80 and student B had a score of  70 out of 100. We would be tempted to simply state that student A performed worse because 50 divided by 80 is `r round(50/80, 3)` while 70 divided by 100 is `r round(70/100, 2)`. However, the tests could have differed in how hard or difficult they were. 

Assuming the students in the two classes are equally intelligent, we can calculate a z-score which allows us to see how likely a score is relative to the mean and standard deviation of a sample (in this case the scores of the other students in each class). To calculate a z-score, we subtract the mean from the observed score and then divide the result by the standard deviation.

\begin{equation}
z =\frac{x_{i} - \mu} {\sigma}   
\end{equation}   
 
or 

\begin{equation}  
z =\frac{x_{i} - \bar{x}} {s}
\end{equation}

A z-score shows how many standard deviations a score is from the mean (which has a value of 0). Positive numbers indicate that the z-score (and thus the original value) is higher or better than the mean while negative z-scores indicate that the z-score (and thus the original value) is lower or worse than the mean. 

Let's use an example to better understand this. We will continue with the example of the two students but we will also take the scores for their class mates into account. The scores are shown below.

```{r z01, echo=F, eval = T, message=FALSE, warning=FALSE}
library(tidyverse)
library(flextable)
zdf <- data.frame(c(71, 78, 67, 41, 68, 52, 47, 46, 63, 78, 45, 66, 60, 50, 40, 71, 69, 74, 54, 50))
zdf <- zdf %>%
  dplyr::rename(Score_c1 = colnames(.)[1]) %>%
  dplyr::mutate(Score_c2 = c(78, 45, 89, 88, 47, 98, 52, 62, 44, 52, 53, 40, 80, 73, 40, 88, 92, 44, 58, 70))
# display table
zdf %>%
  flextable() %>%
  flextable::set_table_properties(width = .4, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Results of two tests at university (class1 had a maximum score of 80, class2 had a maximum score of 100.")  %>%
  flextable::border_outer()
```

The z-score for the student in class 1 (who scored 50 out of 80) would be calculated as 50 minus `r mean(zdf$Score_c1)` (the mean score of class 1) divided by `r round(sd(zdf$Score_c1), 3)` (the standard deviation of class 1). This would result in a z-score of `r round((50 - mean(zdf$Score_c1)) / sd(zdf$Score_c1), 3)` which means that student A's score is `r round((50 - mean(zdf$Score_c1)) / sd(zdf$Score_c1), 3)` standard deviations below the mean of class1.

\begin{equation}
  z_{studentA} =\frac{50 - `r mean(zdf$Score_c1)`} {`r round(sd(zdf$Score_c1), 3)`} 
  = `r round((50 - mean(zdf$Score_c1)) / sd(zdf$Score_c1), 3)`
\end{equation}


The z-score of student B in class 2 (who scored 70 out of 100) would be calculated as 70 minus `r mean(zdf$Score_c2)` (the mean score of class 2) divided by `r round(sd(zdf$Score_c2), 3)` (the standard deviation of class 2). This would result in a z-score of `r (70 - mean(zdf$Score_c2)) / sd(zdf$Score_c2)`  which means that student B's score is `r round((50 - mean(zdf$Score_c2)) / sd(zdf$Score_c2), 3)` standard deviations below the mean of class2.

\begin{equation}
  z_{studentB} =\frac{70 - `r mean(zdf$Score_c2)`} {`r round(sd(zdf$Score_c2), 3)`} 
  = `r round((50 - mean(zdf$Score_c2)) / sd(zdf$Score_c2), 3)`
\end{equation}

Both students perform below average in their class (this is confirmed by the negative z-scores) and, because the z-score of student A is even lower than the z-score of student B, student A has performed worse compared to student B.

We can also easily calculate the z-scores of both classes by calculating the mean of each class, then calculating the standard deviation of each class and then subtracting each score from the mean and dividing the result by the standard deviation as shown below:

```{r z02a, echo=T, eval = T, message=FALSE, warning=FALSE}
zdf <- zdf %>%
  dplyr::mutate(Mean_c1 = mean(Score_c1),
                SD_c1 = round(sd(Score_c1), 2),
                z_c1 = round((Score_c1-Mean_c1)/SD_c1, 2),
                Mean_c2 = mean(Score_c2),
                SD_c2 = round(sd(Score_c2), 2),
                z_c2 = round((Score_c2-Mean_c2)/SD_c2, 2)) %>%
  dplyr::relocate(Score_c1, Mean_c1, SD_c1, z_c1, Score_c2, Mean_c2, SD_c2, z_c2)
```

```{r z02b, echo=F, eval = T, message=FALSE, warning=FALSE}
zdf %>%
  flextable() %>%
  flextable::set_table_properties(width = .75, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Scores, means, standrad deviations, and z-scores of two tests at university.")  %>%
  flextable::border_outer()
```

The table now shows us for each score if the score is below or above the class average (indicated by positive and negative score) and we can compare each score from both classes to see which score is relatively better or worse across the two classes.

```{r z03, echo=F, eval = F, message=FALSE, warning=FALSE}
zdf %>%
  dplyr::rename(Score_c1 = class1,
                Score_c2 = class2) %>%
  dplyr::mutate(Mean_c1 = mean(Score_c1),
                SD_c1 = round(sd(Score_c1), 2),
                z_c1 = round((Score_c1-Mean_c1)/SD_c1, 2),
                Mean_c2 = mean(Score_c2),
                SD_c2 = round(sd(Score_c2), 2),
                z_c2 = round((Score_c2-Mean_c2)/SD_c2, 2)) %>%
  dplyr::relocate(Score_c1, Mean_c1, SD_c1, z_c1, Score_c2, Mean_c2, SD_c2, z_c2) %>%
  ggplot(aes(x = Score_c1)) +
  geom_histogram(binwidth = 5)
```

## Alpha and Beta Errors

One practice that unfortunately still very frequent, which is a very serious problem in data analysis, and which has led to the development of multivariate techniques is the increase of the error rates in multiple or repeated testing. 

We have stated before that we usually assume a significance level of 5%. However, this also means that, on average, every 20^th^ test result, which has a significance value of .05, is misinterpreted because, on average, one out of 20 results shown to be significant *is actually not caused by a real effect but the result of normally distributed probabilities and a fixed significance level*. If we perform several tests, the probability that we obtain a significant result for something which is, in fact, not significant adds up and increases exponentially. Indeed, even with only four tests the likelihood of a significant result in the test - although there is in actually no difference - is 18.5%! This increase in error rates can by easily calculated with formula below.


\begin{equation}

1 - .95^{n} = error

\label{eq:inflatederrors}

\end{equation}

\begin{equation}

1 - .95^{4} = 1 - 0.814 = 0.185

\label{eq:inflatederrorsbsp}

\end{equation}

We will return to this later, but first we will look at different types of errors.

One differentiates between $\alpha$- (or alpha-) and $\beta$ (or beta-) errors. $\alpha$ errors represent a situation in which a test reports a significant effect although there is no effect in empirical reality. $\beta$ errors reflect a situation in which a test does not report a significant effect although there is one in empirical reality (see Table below).

```{r echo=F, message=FALSE, warning=FALSE}
h1 <- c("", "", "Data", "Data")
h2 <- c("", "", "Correlation", "NoCorrelation")
c1 <- c("Reality", "Correlation", "correct", "beta-error")
c2 <- c("Reality", "NoCorrelation", "alpha-error", "correct")
dat <- data.frame(h1, h2, c1, c2)
dat %>%
  flextable() %>%
  flextable::merge_at(i = 1, j = 3:4, part = "body") %>%
  flextable::merge_at(i = 3:4, j = 1, part = "body") %>%
  flextable::delete_part(part = "header") %>%
  flextable::theme_zebra() %>%
  flextable::theme_box() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Alpha- and beta-errors.")  %>%
  flextable::border_outer() %>%
  flextable::bold(i = 1:2, j = 3:4, bold = TRUE, part = "body") %>%
  flextable::bold(i = 3:4, j = 1:2, bold = TRUE, part = "body") %>%
  flextable::color(color = "red", i = 3, j = 4) %>%
  flextable::color(color = "red", i = 4, j = 3) %>%
  flextable::merge_at(i = 1:2, j = 1:2, part = "body") 
```


Regarding the difference between $\alpha$ and $\beta$ errors, it can be said that $\beta$ errors are generally to be preferred, as they merely state that, based on the data, it can not be assumed that X or Y is the case, while $\alpha$ errors do not false statements become part of recognized knowledge. As a rule of thumb, more conservative and conservative behavior is less problematic in terms of science theory, and thus $\alpha$ rather than $\beta$ errors should be avoided.

Now that we have clarified what types of errors exist and that errors accumulate, we will examine a related concept: *Independence*.

## Independence

If errors would always add up, then statistics would not be possible, since every new test would have to take all previous tests into account. This is obviously absurd and cannot be the case. The question now arises about what determines, if errors accumulate or not? The answer is called *independence*.

```{r dep, echo=FALSE, out.width= "30%", out.extra='style="float:right; padding:10px"'}
Subjects <- paste0("Subject ", 1:10)
Scores <- sample(20:50, 10)
df <- data.frame(Subjects, Scores)
df %>%
  flextable() %>%
  flextable::set_table_properties(width = .35, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Independent data - each score comes from a different subject.")  %>%
  flextable::border_outer()
```


If tests are independent of one another, then their errors do not accumulate. However, if they are related to each other, then the errors add up. A word of caution is in order here as the concept of independence in statistics has a different meaning from everyday use: in statistics, independence means the independence of hypotheses. In the case of specifications of more general hypotheses, the specified hypotheses are not independent of the general hypothesis and are not independent of the other specified hypotheses. In other words, if we test several specific hypotheses in a more generalized hypothesis, then the hypotheses are not strictly independent and cannot be treated that way. If we formulate two hypotheses that are not conceptually linked or one hypothesis is not derived from the other, then these hypotheses are independent. 

We encounter a related phenomenon when dealing with extensions of $\chi$^2^ test: the reason why we could not calculate the ordinary Pearson's $\chi$^2^ test when focusing on sub-tables derived from larger tables is that the data of represented in the sub-table is not independent from the other observations summarized in the larger table. 

```{r indep, echo=FALSE, out.width= "30%", out.extra='style="float:right; padding:10px"'}
Subjects <- rep(paste0("Subject ", 1:2), each = 5)
Scores <- sample(20:50, 10)
df <- data.frame(Subjects, Scores)
df %>%
  flextable() %>%
  flextable::set_table_properties(width = .35, layout = "autofit") %>%
  flextable::theme_zebra() %>%
  flextable::fontsize(size = 12) %>%
  flextable::fontsize(size = 12, part = "header") %>%
  flextable::align_text_col(align = "center") %>%
  flextable::set_caption(caption = "Dependent data - several scores come from the same subject.")  %>%
  flextable::border_outer()
```

When we performed the ordinary Pearson's $\chi$^2^ test we tested whether emotion metaphors are realized differently across registers. The subsequent test built on the results of that first test and zoomed in on a specified hypothesis (namely that two specific types of metaphors would differ in two specific registers). Therefore, we are dealing with two hypotheses, the second hypothesis being a specification of the first hypothesis. This means that the hypotheses were related and not independent. Consequently, the errors would have added up if we had not considered that not only the part table was extracted from the data, but we wanted to test a part table of a larger table.

A second and perhaps more important feature of independence is that independent variables must not be correlated. If they are, however, this is called *collinearity* (or *multicollinearity*) and they are referred to as being *collinear* (more on this when we look at multiple regression).

## Corrections

Now that we know about error accumulation and issues of independence, how can we test multiple hypotheses (simultaneously)? One option is to use multivariate methods, as we will see in the section on "Advanced Statistics". Another option is to incorporate corrections to ensure that the $\alpha$-level remains at 5% even with repeated or multiple testing.

The best known and probably the most widely used correction is the *Bonferroni* correction, where the $\alpha$-level is divided by the number of tests. For example, if we perform 4 tests, then the $\alpha$-level is lowered to .05 / 4 = .0125 so that the $\alpha$-level of the four tests returns to the usual 5% level. The disadvantage of this correction is that it is more conservative and therefore leads to a relatively high $\beta$-error rate.

Other common corrections are the Holm and the Benjamini-Hochberg corrections. However, we will not discuss them here. The interested reader is referred to @field2012discovering, pp. 429-430. 


[Back to top](#basic-concepts_in_quantitative_research_2)



