[["multiple-linear-regression-1.html", "Week 10 Multiple Linear Regression 10.1 The Basics of Multiple Linear Regression 10.2 Example: Gifts and Availability 10.3 (Automatic) Model Fitting 10.4 Model Diagnostics 10.5 Evaluation of Sample Size", " Week 10 Multiple Linear Regression This week, we continue with regression analysis and expand on last week by including multiple predictors. Preparation and session set up For this week’s content, we need to install certain packages from an R library so that the scripts shown below are executed without errors. hence, before turning to the code below, please install the packages by running the code below this paragraph - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time). # install install.packages(&quot;car&quot;) install.packages(&quot;flextable&quot;) install.packages(&quot;ggplot2&quot;) install.packages(&quot;Hmisc&quot;) install.packages(&quot;knitr&quot;) install.packages(&quot;MASS&quot;) install.packages(&quot;rms&quot;) install.packages(&quot;sjPlot&quot;) install.packages(&quot;stringr&quot;) install.packages(&quot;report&quot;) install.packages(&quot;dplyr&quot;) install.packages(&quot;vip&quot;) # install klippy for copy-to-clipboard button in code chunks install.packages(&quot;remotes&quot;) remotes::install_github(&quot;rlesur/klippy&quot;) Now that we have installed the packages, we activate them as shown below. # set options options(stringsAsFactors = F) # no automatic data transformation options(&quot;scipen&quot; = 100, &quot;digits&quot; = 12) # suppress math annotation # load packages library(car) library(dplyr) library(flextable) library(ggfortify) library(ggplot2) library(report) library(rms) library(sjPlot) library(stringr) library(vip) # activate klippy for copy-to-clipboard button klippy::klippy() Once you have installed R and RStudio and initiated the session by executing the code shown above, you are good to go. 10.1 The Basics of Multiple Linear Regression In contrast to simple linear regression, which estimates the effect of a single predictor, multiple linear regression estimates the effect of various predictors simultaneously (see the equation below). \\[\\begin{equation} f_{(x)} = \\alpha + \\beta_{1}x_{i} + \\beta_{2}x_{i+1} + \\dots + \\beta_{n}x_{i+n} + \\epsilon \\end{equation}\\] There exists a wealth of literature focusing on multiple linear regressions and the concepts it is based on. For instance, there are Achen (1982), Bortz (2006), Crawley (2005), Faraway (2002), Field, Miles, and Field (2012), Gries (2021), Levshina (2015), Wilcox (2009), and Winter (2019) to name just a few. Introductions to regression modeling in R are Baayen (2008), Crawley (2012), Gries (2021), Levshina (2015), and Winter (2019). The model diagnostics we are dealing with here are partly identical to the diagnostic methods discussed last week when wefocused on simple linear regression. Because of this overlap, diagnostics will only be described in more detail if they have not been described in the section on simple linear regression. 10.2 Example: Gifts and Availability The example we will go through here is taken from Field, Miles, and Field (2012). In this example, the research question is if the money that men spend on presents for women depends on the women’s attractiveness and their relationship status. To answer this research question, we will implement a multiple linear regression and start by loading the data and inspect its structure and properties. # load data mlrdata &lt;- base::readRDS(url(&quot;https://slcladal.github.io/data/mld.rda&quot;, &quot;rb&quot;)) .tabwid table{ border-spacing:0px !important; border-collapse:collapse; line-height:1; margin-left:auto; margin-right:auto; border-width: 0; display: table; margin-top: 1.275em; margin-bottom: 1.275em; border-color: transparent; } .tabwid_left table{ margin-left:0; } .tabwid_right table{ margin-right:0; } .tabwid td { padding: 0; } .tabwid a { text-decoration: none; } .tabwid thead { background-color: transparent; } .tabwid tfoot { background-color: transparent; } .tabwid table tr { background-color: transparent; } .cl-3b0d9022{table-layout:auto;width:75%;}.cl-3b01ae6a{font-family:'Arial';font-size:12pt;font-weight:bold;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3b01ae7e{font-family:'Arial';font-size:12pt;font-weight:normal;font-style:normal;text-decoration:none;color:rgba(0, 0, 0, 1.00);background-color:transparent;}.cl-3b01d7be{margin:0;text-align:center;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b01d7c8{margin:0;text-align:right;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);padding-bottom:5pt;padding-top:5pt;padding-left:5pt;padding-right:5pt;line-height: 1;background-color:transparent;}.cl-3b0257de{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b0257e8{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b0257f2{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b0257f3{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b0257f4{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b0257f5{background-color:transparent;vertical-align: middle;border-bottom: 0 solid rgba(0, 0, 0, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b0257fc{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b0257fd{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b0257fe{background-color:rgba(239, 239, 239, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 0 solid rgba(0, 0, 0, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b025806{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b025807{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 1pt solid rgba(102, 102, 102, 1.00);border-right: 0 solid rgba(0, 0, 0, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;}.cl-3b025810{background-color:rgba(207, 207, 207, 1.00);vertical-align: middle;border-bottom: 1pt solid rgba(102, 102, 102, 1.00);border-top: 1pt solid rgba(102, 102, 102, 1.00);border-left: 0 solid rgba(0, 0, 0, 1.00);border-right: 1pt solid rgba(102, 102, 102, 1.00);margin-bottom:0;margin-top:0;margin-left:0;margin-right:0;} Table 10.1: First 15 rows of the mlrdata. statusattractionmoneyRelationshipNotInterested86.33RelationshipNotInterested45.58RelationshipNotInterested68.43RelationshipNotInterested52.93RelationshipNotInterested61.86RelationshipNotInterested48.47RelationshipNotInterested32.79RelationshipNotInterested35.91RelationshipNotInterested30.98RelationshipNotInterested44.82RelationshipNotInterested35.05RelationshipNotInterested64.49RelationshipNotInterested54.50RelationshipNotInterested61.48RelationshipNotInterested55.51 The data set consist of three variables stored in three columns. The first column contains the relationship status of the present giver (in this study this were men), the second whether the man is interested in the woman (the present receiver in this study), and the third column represents the money spend on the present. The data set represents 100 cases and the mean amount of money spend on a present is 88.38 dollars. In a next step, we visualize the data to get a more detailed impression of the relationships between variables. # create plots p1 &lt;- ggplot(mlrdata, aes(status, money)) + geom_boxplot() + theme_bw() # plot 2 p2 &lt;- ggplot(mlrdata, aes(attraction, money)) + geom_boxplot() + theme_bw() # plot 3 p3 &lt;- ggplot(mlrdata, aes(x = money)) + geom_histogram(aes(y=..density..)) + theme_bw() + geom_density() # plot 4 p4 &lt;- ggplot(mlrdata, aes(status, money)) + geom_boxplot(aes(fill = factor(status))) + facet_wrap(~ attraction) + guides(fill = FALSE) + theme_bw() # show plots vip::grid.arrange(grobs = list(p1, p2, p3, p4), widths = c(1, 1), layout_matrix = rbind(c(1, 2), c(3, 4))) The upper left figure consists of a boxplot which shows how much money was spent by relationship status. The figure suggests that men spend more on women if they are not in a relationship. The next figure shows the relationship between the money spend on presents and whether or not the men were interested in the women. The boxplot in the upper right panel suggests that men spend substantially more on women if the men are interested in them. The next figure depicts the distribution of the amounts of money spend on the presents for the women. In addition, the figure indicates the existence of two outliers (dots in the boxplot) The histogram in the lower left panel shows that, although the mean amount of money spent on presents is 88.38 dollars, the distribution peaks around 50 dollars indicating that on average, men spend about 50 dollars on presents. Finally, we will plot the amount of money spend on presents against relationship status by attraction in order to check whether the money spent on presents is affected by an interaction between attraction and relationship status. The boxplot in the lower right panel confirms the existence of an interaction (a non-additive term) as men only spend more money on women if the men single and they are interested in the women. If men are not interested in the women, then the relationship has no effect as they spend an equal amount of money on the women regardless of whether they are in a relationship or not. We will now start to implement the regression model. In a first step, we create a saturated model that contain all possible predictors (main effects and interactions). m1.mlr = lm( # generate lm regression object money ~ attraction*status, # def. regression formula data = mlrdata) # def. data After generating the saturated models we can now start with the model fitting. Model fitting refers to a process that aims at find the model that explains a maximum of variance with a minimum of predictors (see Field, Miles, and Field 2012, 318). Model fitting is therefore based on the principle of parsimony which is related to Occam’s razor according to which explanations that require fewer assumptions are more likely to be true. 10.3 (Automatic) Model Fitting Model fitting means that we check what of the predictors should be included in the model and what predictors do not have any impact on the outcome and thus should not be part of the model (because they merely add noise to the data. In this example, we use a step-wise step-down procedure that uses decreases in AIC (Akaike Information Criterion) as the criterion to minimize the model in a step-wise manner. This procedure aims at finding the model with the lowest AIC values by evaluating - step-by-step - whether the removal of a predictor (term) leads to a lower AIC value. Other options for model fitting would, e.g., be step-wise step-up or forced entry. We will use step-wise step-up in another week and if you want to find out more about model fitting procedures, check ou Field, Miles, and Field (2012), 264-265. The AIC is calculated using the equation below. The lower the AIC value, the better the balance between explained variance and the number of predictors. AIC values can and should only be compared for models that are fit on the same data set with the same (number of) cases (LL stands for logged likelihood or LogLikelihood and k represents the number of predictors in the model (including the intercept); the LL represents a measure of how good the model fits the data). \\[\\begin{equation} Akaike Information Criterion (AIC) = -2LL + 2k \\end{equation}\\] When fitting a model step-wise step-down, interactions are evaluated first and only if all insignificant interactions have been removed would the procedure start removing insignificant main effects (that are not part of significant interactions). Other model fitting procedures (forced entry, step-wise step up, hierarchical) are discussed during the implementation of other regression models. We cannot discuss all procedures here as model fitting is rather complex and a discussion of even the most common procedures would to lengthy and time consuming at this point. It is important to note though that there is not perfect model fitting procedure and automated approaches should be handled with care as they are likely to ignore violations of model parameters that can be detected during manual - but time consuming - model fitting procedures. As a general rule of thumb, it is advisable to fit models as carefully and deliberately as possible. We will now begin to fit the model. # automated AIC based model fitting step(m1.mlr, direction = &quot;both&quot;) ## Start: AIC=592.52 ## money ~ attraction * status ## ## Df Sum of Sq RSS AIC ## &lt;none&gt; 34557.56428 592.5211556 ## - attraction:status 1 24947.25481 59504.81909 644.8642395 ## ## Call: ## lm(formula = money ~ attraction * status, data = mlrdata) ## ## Coefficients: ## (Intercept) attractionNotInterested ## 99.1548 -47.6628 ## statusSingle attractionNotInterested:statusSingle ## 57.6928 -63.1788 The automated model fitting procedure informs us that removing predictors has not caused a decrease in the AIC. The saturated model is thus also the final minimal adequate model. We will now inspect the final minimal model and go over the model report. m2.mlr = lm( # generate lm regression object money ~ (status + attraction)^2, # def. regression formula data = mlrdata) # def. data # inspect final minimal model summary(m2.mlr) ## ## Call: ## lm(formula = money ~ (status + attraction)^2, data = mlrdata) ## ## Residuals: ## Min 1Q Median 3Q Max ## -45.0760 -14.2580 0.4596 11.9315 44.1424 ## ## Coefficients: ## Estimate Std. Error t value ## (Intercept) 99.15480000 3.79459947 26.13050 ## statusSingle 57.69280000 5.36637403 10.75080 ## attractionNotInterested -47.66280000 5.36637403 -8.88175 ## statusSingle:attractionNotInterested -63.17880000 7.58919893 -8.32483 ## Pr(&gt;|t|) ## (Intercept) &lt; 0.000000000000000222 *** ## statusSingle &lt; 0.000000000000000222 *** ## attractionNotInterested 0.00000000000003751 *** ## statusSingle:attractionNotInterested 0.00000000000058085 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.9729973 on 96 degrees of freedom ## Multiple R-squared: 0.852041334, Adjusted R-squared: 0.847417626 ## F-statistic: 184.276619 on 3 and 96 DF, p-value: &lt; 0.0000000000000002220446 The first element of the report is called Call and it reports the regression formula of the model. Then, the report provides the residual distribution (the range, median and quartiles of the residuals) which allows drawing inferences about the distribution of differences between observed and expected values. If the residuals are distributed non-normally, then this is a strong indicator that the model is unstable and unreliable because mathematical assumptions on which the model is based are violated. Next, the model summary reports the most important part: a table with model statistics of the fixed-effects structure of the model. The table contains the estimates (coefficients of the predictors), standard errors, t-values, and the p-values which show whether a predictor significantly correlates with the dependent variable that the model investigates. All main effects (status and attraction) as well as the interaction between status and attraction is reported as being significantly correlated with the dependent variable (money). An interaction occurs if a correlation between the dependent variable and a predictor is affected by another predictor. The top most term is called intercept and has a value of 99.15 which represents the base estimate to which all other estimates refer. To exemplify what this means, let us consider what the model would predict that a man would spend on a present if he interested in the woman but he is also in a relationship. The amount he would spend (based on the model would be 99.15 dollars (which is the intercept). This means that the intercept represents the predicted value if all predictors take the base or reference level. And since being in relationship but being interested are the case, and because the interaction does not apply, the predicted value in our example is exactly the intercept (see below). #intercept Single NotInterested Single:NotInterested 99.15 + 57.69 + 0 + 0 # 156.8 single + interested ## [1] 156.84 99.15 + 57.69 - 47.66 - 63.18 # 46.00 single + not interested ## [1] 46 99.15 - 0 + 0 - 0 # 99.15 relationship + interested ## [1] 99.15 99.15 - 0 - 47.66 - 0 # 51.49 relationship + not interested ## [1] 51.49 Now, let us consider what a man would spend if he is in a relationship and he is not attracted to the women. In that case, the model predicts that the man would spend only 51.49 dollars on a present: the intercept (99.15) minus 47.66 because the man is not interested (and no additional subtraction because the interaction does not apply). We can derive the same results easier using the predict function. # make prediction based on the model for original data prediction &lt;- predict(m2.mlr, newdata = mlrdata) # inspect predictions table(round(prediction,2)) ## ## 46.01 51.49 99.15 156.85 ## 25 25 25 25 Below the table of coefficients, the regression summary reports model statistics that provide information about how well the model performs. The difference between the values and the values in the coefficients table is that the model statistics refer to the model as a whole rather than focusing on individual predictors. The multiple R2-value is a measure of how much variance the model explains. A multiple R2-value of 0 would inform us that the model does not explain any variance while a value of .852 mean that the model explains 85.2 percent of the variance. A value of 1 would inform us that the model explains 100 percent of the variance and that the predictions of the model match the observed values perfectly. Multiplying the multiple R2-value thus provides the percentage of explained variance. Models that have a multiple R2-value equal or higher than .05 are deemed substantially significant (see Szmrecsanyi 2006, 55). It has been claimed that models should explain a minimum of 5 percent of variance but this is problematic as it is not uncommon for models to have very low explanatory power while still performing significantly and systematically better than chance. In addition, the total amount of variance is negligible in cases where one is interested in very weak but significant effects. It is much more important for model to perform significantly better than minimal base-line models because if this is not the case, then the model does not have any predictive and therefore no explanatory power. The adjusted R2-value considers the amount of explained variance in light of the number of predictors in the model (it is thus somewhat similar to the AIC and BIC) and informs about how well the model would perform if it were applied to the population that the sample is drawn from. Ideally, the difference between multiple and adjusted R2-value should be very small as this means that the model is not overfitted. If, however, the difference between multiple and adjusted R2-value is substantial, then this would strongly suggest that the model is unstable and overfitted to the data while being inadequate for drawing inferences about the population. Differences between multiple and adjusted R2-values indicate that the data contains outliers that cause the distribution of the data on which the model is based to differ from the distributions that the model mathematically requires to provide reliable estimates. The difference between multiple and adjusted R2-value in our model is very small (85.2-84.7=.05) and should not cause concern. Before continuing, we will calculate the confidence intervals of the coefficients. # extract confidence intervals of the coefficients confint(m2.mlr) ## 2.5 % 97.5 % ## (Intercept) 91.6225795890 106.6870204110 ## statusSingle 47.0406317400 68.3449682600 ## attractionNotInterested -58.3149682600 -37.0106317400 ## statusSingle:attractionNotInterested -78.2432408219 -48.1143591781 # create and compare baseline- and minimal adequate model m0.mlr &lt;- lm(money ~1, data = mlrdata) anova(m0.mlr, m2.mlr) ## Analysis of Variance Table ## ## Model 1: money ~ 1 ## Model 2: money ~ (status + attraction)^2 ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 99 233562.28650 ## 2 96 34557.56428 3 199004.7222 184.27662 &lt; 0.000000000000000222 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 Now, we compare the final minimal adequate model to the base-line model to test whether then final model significantly outperforms the baseline model. # compare baseline- and minimal adequate model Anova(m0.mlr, m2.mlr, type = &quot;III&quot;) ## Anova Table (Type III tests) ## ## Response: money ## Sum Sq Df F value Pr(&gt;F) ## (Intercept) 781015.8300 1 2169.64133 &lt; 0.000000000000000222 *** ## Residuals 34557.5643 96 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 The comparison between the two model confirms that the minimal adequate model performs significantly better (makes significantly more accurate estimates of the outcome variable) compared with the baseline model. 10.4 Model Diagnostics After implementing the multiple regression, we now need to look for outliers and perform the model diagnostics by testing whether removing data points disproportionately decreases model fit. To begin with, we generate diagnostic plots. # generate plots autoplot(m2.mlr) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank()) + theme_bw() The plots do not show severe problems such as funnel shaped patterns or drastic deviations from the diagonal line in Normal Q-Q plot (have a look at the explanation of what to look for and how to interpret these diagnostic plots in the section on simple linear regression) but data points 52, 64, and 83 are repeatedly indicated as potential outliers. # determine a cutoff for data points that have D-values higher than 4/(n-k-1) cutoff &lt;- 4/((nrow(mlrdata)-length(m2.mlr$coefficients)-2)) # start plotting par(mfrow = c(1, 2)) # display plots in 3 rows/2 columns qqPlot(m2.mlr, main=&quot;QQ Plot&quot;) # create qq-plot ## [1] 52 83 plot(m2.mlr, which=4, cook.levels = cutoff); par(mfrow = c(1, 1)) The graphs indicate that data points 52, 64, and 83 may be problematic but they do not seem to overly affect the results and we will thus ignore this issue here. In a real analysis, you would take the data points out, re-run the regression and see if removing the data points leads to noticeably different results. If the results are not substantively different, I would suggest to keep these data points in as simply removing data points is not a good practice (see below). NOTEIn general, outliers should not simply be removed unless there are good reasons for it (this could be that the outliers represent measurement errors). If a data set contains outliers, one should rather switch to methods that are better at handling outliers, e.g. by using weights to account for data points with high leverage. One alternative would be to switch to a robust regression (see here). However, here we show how to proceed by removing outliers as this is a common, though potentially problematic, method of dealing with outliers. When the diagnostic plots indicate potential outliers the following parameters should be considered: Data points with standardized residuals &gt; 3.29 should be removed (Field, Miles, and Field 2012, 269) If more than 1 percent of data points have standardized residuals exceeding values &gt; 2.58, then the error rate of the model is unacceptable (Field, Miles, and Field 2012, 269). If more than 5 percent of data points have standardized residuals exceeding values &gt; 1.96, then the error rate of the model is unacceptable (Field, Miles, and Field 2012, 269) In addition, data points with Cook’s D-values &gt; 1 should be removed (Field, Miles, and Field 2012, 269) Also, data points with leverage values higher than \\(3(k + 1)/N\\) or \\(2(k + 1)/N\\) (k = Number of predictors, N = Number of cases in model) should be removed (Field, Miles, and Field 2012, 270) There should not be (any) autocorrelation among predictors. This means that independent variables cannot be correlated with itself (for instance, because data points come from the same subject). If there is autocorrelation among predictors, then a Repeated Measures Design or a (hierarchical) mixed-effects model should be implemented instead. Predictors cannot substantially correlate with each other (multicollinearity) (see the subsection on (multi-)collinearity in the section of multiple binomial logistic regression for more details about (multi-)collinearity). If a model contains predictors that have variance inflation factors (VIF) &gt; 10 the model is unreliable (Myers 1990) and predictors causing such VIFs should be removed. Indeed, even VIFs of 2.5 can be problematic (Szmrecsanyi 2006, 215) Indeed, Zuur, Ieno, and Elphick (2010) propose that variables with VIFs exceeding 3 should be removed! NOTEHowever, (multi-)collinearity is only an issue if one is interested in interpreting regression results! If the interpretation is irrelevant because what is relevant is prediction(!), then it does not matter if the model contains collinear predictors! See Gries (2021) for a more elaborate explanation. The mean value of VIFs should be ~ 1 (Bowerman and O’Connell 1990). The following code chunk creates and evaluates these criteria. # add model diagnostics to the data mlrdata &lt;- mlrdata %&gt;% dplyr::mutate(residuals = resid(m2.mlr), standardized.residuals = rstandard(m2.mlr), studentized.residuals = rstudent(m2.mlr), cooks.distance = cooks.distance(m2.mlr), dffit = dffits(m2.mlr), leverage = hatvalues(m2.mlr), covariance.ratios = covratio(m2.mlr), fitted = m2.mlr$fitted.values) Visually assessing the model fit. # plot 5 p5 &lt;- ggplot(mlrdata, aes(studentized.residuals)) + theme(legend.position = &quot;none&quot;)+ geom_histogram(aes(y=..density..), binwidth = .2, colour=&quot;black&quot;, fill=&quot;gray90&quot;) + labs(x = &quot;Studentized Residual&quot;, y = &quot;Density&quot;) + stat_function(fun = dnorm, args = list(mean = mean(mlrdata$studentized.residuals, na.rm = TRUE), sd = sd(mlrdata$studentized.residuals, na.rm = TRUE)), colour = &quot;red&quot;, size = 1) + theme_bw(base_size = 8) # plot 6 p6 &lt;- ggplot(mlrdata, aes(fitted, studentized.residuals)) + geom_point() + geom_smooth(method = &quot;lm&quot;, colour = &quot;Red&quot;)+ theme_bw(base_size = 8)+ labs(x = &quot;Fitted Values&quot;, y = &quot;Studentized Residual&quot;) # plot 7 p7 &lt;- qplot(sample = mlrdata$studentized.residuals, stat=&quot;qq&quot;) + theme_bw(base_size = 8) + labs(x = &quot;Theoretical Values&quot;, y = &quot;Observed Values&quot;) ## Warning: `stat` is deprecated vip::grid.arrange(p5, p6, p7, nrow = 1) ## `geom_smooth()` using formula &#39;y ~ x&#39; Statistically assessing the model fit. # 1: optimal = 0 # (listed data points should be removed) which(mlrdata$standardized.residuals &gt; 3.29) ## named integer(0) # 2: optimal = 1 # (listed data points should be removed) stdres_258 &lt;- as.vector(sapply(mlrdata$standardized.residuals, function(x) { ifelse(sqrt((x^2)) &gt; 2.58, 1, 0) } )) (sum(stdres_258) / length(stdres_258)) * 100 ## [1] 0 # 3: optimal = 5 # (listed data points should be removed) stdres_196 &lt;- as.vector(sapply(mlrdata$standardized.residuals, function(x) { ifelse(sqrt((x^2)) &gt; 1.96, 1, 0) } )) (sum(stdres_196) / length(stdres_196)) * 100 ## [1] 5 # 4: optimal = 0 # (listed data points should be removed) which(mlrdata$cooks.distance &gt; 1) ## named integer(0) # 5: optimal = 0 # (data points should be removed if cooks distance is close to 1) which(mlrdata$leverage &gt;= (3*mean(mlrdata$leverage))) ## named integer(0) # 6: checking autocorrelation: # Durbin-Watson test (optimal: high p-value) dwt(m2.mlr) ## lag Autocorrelation D-W Statistic p-value ## 1 0.0225138026771 1.89885384063 0.406 ## Alternative hypothesis: rho != 0 # 7: test multicollinearity 1 vif(m2.mlr) ## statusSingle attractionNotInterested ## 2 2 ## statusSingle:attractionNotInterested ## 3 # 8: test multicollinearity 2 1/vif(m2.mlr) ## statusSingle attractionNotInterested ## 0.500000000000 0.500000000000 ## statusSingle:attractionNotInterested ## 0.333333333333 # 9: mean vif should not exceed 1 mean(vif(m2.mlr)) ## [1] 2.33333333333 Except for the mean VIF value (2.307) which should not exceed 1, all diagnostics are acceptable. We will now test whether the sample size is sufficient for our model. With respect to the minimal sample size and based on Green (1991), Field, Miles, and Field (2012, 273–74) offer the following rules of thumb for an adequate sample size (k = number of predictors; categorical predictors with more than two levels should be recoded as dummy variables): if you are interested in the overall model: 50 + 8k (k = number of predictors) if you are interested in individual predictors: 104 + k if you are interested in both: take the higher value! 10.5 Evaluation of Sample Size After performing the diagnostics, we will now test whether the sample size is adequate and what the values of R would be based on a random distribution in order to be able to estimate how likely a \\(\\beta\\)-error is given the present sample size (see Field, Miles, and Field 2012, 274). Beta errors (or \\(\\beta\\)-errors) refer to the erroneous assumption that a predictor is not significant (based on the analysis and given the sample) although it does have an effect in the population. In other words, \\(\\beta\\)-error means to overlook a significant effect because of weaknesses of the analysis. The test statistics ranges between 0 and 1 where lower values are better. If the values approximate 1, then there is serious concern as the model is not reliable given the sample size. In such cases, unfortunately, the best option is to increase the sample size. Despite there being no ultimate rule of thumb, Field, Miles, and Field (2012, 273–75), based on Green (1991), provide data-driven suggestions for the minimal size of data required for regression models that aim to find medium sized effects (k = number of predictors; categorical variables with more than two levels should be transformed into dummy variables): If one is merely interested in the overall model fit (something I have not encountered), then the sample size should be at least 50 + k (k = number of predictors in model). If one is only interested in the effect of specific variables, then the sample size should be at least 104 + k (k = number of predictors in model). If one is only interested in both model fit and the effect of specific variables, then the sample size should be at least the higher value of 50 + k or 104 + k (k = number of predictors in model). You will see in the R code below that there is already a function that tests whether the sample size is sufficient. # load functions source(&quot;https://slcladal.github.io/rscripts/SampleSizeMLR.r&quot;) source(&quot;https://slcladal.github.io/rscripts/ExpR.r&quot;) # check if sample size is sufficient smplesz(m2.mlr) ## [1] &quot;Sample too small: please increase your sample by 7 data points&quot; # check beta-error likelihood expR(m2.mlr) ## [1] &quot;Based on the sample size expect a false positive correlation of 0.0303 between the predictors and the predicted&quot; The function smplesz reports that the sample size is insufficient by 9 data points according to Green (1991). The likelihood of \\(\\beta\\)-errors, however, is very small (0.0309). EXCURSION ` A note on sample size and power A brief note on minimum necessary sample or data set size appears necessary here. Although there appears to be a general assumption that 25 data points per group are sufficient, this is not necessarily correct (it is merely a general rule of thumb that is actually often incorrect). Such rules of thumb are inadequate because the required sample size depends on the number of variables in a given model, the size of the effect and the variance of the effect - in other words, the minimum necessary sample size relates to statistical power (see here for a tutorial on power). If a model contains many variables, then this requires a larger sample size than a model which only uses very few predictors. Also, to detect an effect with a very minor effect size, one needs a substantially larger sample compared to cases where the effect is very strong. In fact, when dealing with small effects, model require a minimum of 600 cases to reliably detect these effects. Finally, effects that are very robust and do not vary much require a much smaller sample size compared with effects that are spurious and vary substantially. Since the sample size depends on the effect size and variance as well as the number of variables, there is no final one-size-fits-all answer to what the best sample size is. Another, slightly better but still incorrect, rule of thumb is that the more data, the better. This is not correct because models based on too many cases are prone for overfitting and thus report correlations as being significant that are not. However, given that there are procedures that can correct for overfitting, larger data sets are still preferable to data sets that are simply too small to warrant reliable results. In conclusion, it remains true that the sample size depends on the effect under investigation. ` As a last step, we summarize the results of the regression analysis. # tabulate model results sjPlot::tab_model(m2.mlr)   money Predictors Estimates CI p (Intercept) 99.15 91.62 – 106.69 &lt;0.001 status [Single] 57.69 47.04 – 68.34 &lt;0.001 attraction[NotInterested] -47.66 -58.31 – -37.01 &lt;0.001 status [Single] *attraction[NotInterested] -63.18 -78.24 – -48.11 &lt;0.001 Observations 100 R2 / R2 adjusted 0.852 / 0.847 NOTEThe R2 values in this report is incorrect! As we have seen above, and is also shown in the table below, the correct R2 values are: multiple R2 0.8574, adjusted R2 0.8528. Additionally, we can inspect the summary of the regression model as shown below to extract additional information. We can use the reports package (Makowski et al. 2021) to summarize the analysis. Although Field, Miles, and Field (2012) suggest that the main effects of the predictors involved in the interaction should not be interpreted, they are interpreted here to illustrate how the results of a multiple linear regression can be reported. report::report(m2.mlr) ## We fitted a linear model (estimated using OLS) to predict money with status and attraction (formula: money ~ (status + attraction)^2). The model explains a statistically significant and substantial proportion of variance (R2 = 0.85, F(3, 96) = 184.28, p &lt; .001, adj. R2 = 0.85). The model&#39;s intercept, corresponding to status = Relationship and attraction = Interested, is at 99.15 (95% CI [91.62, 106.69], t(96) = 26.13, p &lt; .001). Within this model: ## ## - The effect of status [Single] is statistically significant and positive (beta = 57.69, 95% CI [47.04, 68.34], t(96) = 10.75, p &lt; .001; Std. beta = 1.19, 95% CI [0.97, 1.41]) ## - The effect of attraction [NotInterested] is statistically significant and negative (beta = -47.66, 95% CI [-58.31, -37.01], t(96) = -8.88, p &lt; .001; Std. beta = -0.98, 95% CI [-1.20, -0.76]) ## - The interaction effect of attraction [NotInterested] on status [Single] is statistically significant and negative (beta = -63.18, 95% CI [-78.24, -48.11], t(96) = -8.32, p &lt; .001; Std. beta = -1.30, 95% CI [-1.61, -0.99]) ## ## Standardized parameters were obtained by fitting the model on a standardized version of the dataset. 95% Confidence Intervals (CIs) and p-values were computed using the Wald approximation. Although Field, Miles, and Field (2012) suggest that the main effects of the predictors involved in the interaction should not be interpreted, they are interpreted here to illustrate how the results of a multiple linear regression can be reported. We can use the output of the report function to write up a final report: A multiple linear regression was fitted to the data using an automated, step-wise, AIC-based (Akaike’s Information Criterion) procedure. The model fitting arrived at a final minimal model. During the model diagnostics, two outliers were detected and removed. Further diagnostics did not find other issues after the removal. The final minimal adequate regression model is based on 98 data points and performs highly significantly better than a minimal baseline model (multiple R2: .857, adjusted R2: .853, F-statistic (3, 94): 154.4, AIC: 850.4, BIC: 863.32, p&lt;.001\\(***\\)). The final minimal adequate regression model reports attraction and status as significant main effects. The relationship status of men correlates highly significantly and positively with the amount of money spend on the women’s presents (SE: 5.14, t-value: 10.87, p&lt;.001\\(***\\)). This shows that men spend 156.8 dollars on presents if they are single while they spend 99,15 dollars if they are in a relationship. Whether men are attracted to women also correlates highly significantly and positively with the money they spend on women (SE: 5.09, t-values: -9.37, p&lt;.001\\(***\\)). If men are not interested in women, they spend 47.66 dollar less on a present for women compared with women the men are interested in. Furthermore, the final minimal adequate regression model reports a highly significant interaction between relationship status and attraction (SE: 7.27, t-value: -8.18, p&lt;.001\\(***\\)): If men are single but they are not interested in a women, a man would spend only 59.46 dollars on a present compared to all other constellations. sessionInfo() ## R version 4.2.1 RC (2022-06-17 r82510 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 10 x64 (build 19043) ## ## Matrix products: default ## ## locale: ## [1] LC_COLLATE=German_Germany.utf8 LC_CTYPE=German_Germany.utf8 ## [3] LC_MONETARY=German_Germany.utf8 LC_NUMERIC=C ## [5] LC_TIME=German_Germany.utf8 ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] vip_0.3.2 stringr_1.4.0 sjPlot_2.8.10 rms_6.3-0 ## [5] SparseM_1.81 Hmisc_4.7-0 Formula_1.2-4 survival_3.3-1 ## [9] lattice_0.20-45 report_0.5.1 ggfortify_0.4.14 ggplot2_3.3.6 ## [13] flextable_0.7.2 dplyr_1.0.9 car_3.1-0 carData_3.0-5 ## ## loaded via a namespace (and not attached): ## [1] TH.data_1.1-1 minqa_1.2.4 colorspace_2.0-3 ## [4] ellipsis_0.3.2 sjlabelled_1.2.0 estimability_1.3 ## [7] htmlTable_2.4.0 parameters_0.17.0 base64enc_0.1-3 ## [10] rstudioapi_0.13 farver_2.1.0 MatrixModels_0.5-0 ## [13] fansi_1.0.3 mvtnorm_1.1-3 xml2_1.3.3 ## [16] codetools_0.2-18 splines_4.2.1 knitr_1.39 ## [19] sjmisc_2.8.9 jsonlite_1.8.0 nloptr_2.0.1 ## [22] ggeffects_1.1.2 broom_0.8.0 cluster_2.1.3 ## [25] png_0.1-7 effectsize_0.6.0.1 compiler_4.2.1 ## [28] sjstats_0.18.1 emmeans_1.7.3 backports_1.4.1 ## [31] assertthat_0.2.1 Matrix_1.4-1 fastmap_1.1.0 ## [34] cli_3.3.0 htmltools_0.5.2 quantreg_5.93 ## [37] tools_4.2.1 coda_0.19-4 gtable_0.3.0 ## [40] glue_1.6.2 Rcpp_1.0.8.3 jquerylib_0.1.4 ## [43] vctrs_0.4.1 nlme_3.1-157 insight_0.17.1 ## [46] xfun_0.30 lme4_1.1-29 lifecycle_1.0.1 ## [49] klippy_0.0.0.9500 polspline_1.1.20 MASS_7.3-57 ## [52] zoo_1.8-10 scales_1.2.0 sandwich_3.0-1 ## [55] RColorBrewer_1.1-3 yaml_2.3.5 gridExtra_2.3 ## [58] gdtools_0.2.4 sass_0.4.1 rpart_4.1.16 ## [61] latticeExtra_0.6-29 stringi_1.7.6 highr_0.9 ## [64] bayestestR_0.12.1 checkmate_2.1.0 boot_1.3-28 ## [67] zip_2.2.0 rlang_1.0.2 pkgconfig_2.0.3 ## [70] systemfonts_1.0.4 evaluate_0.15 purrr_0.3.4 ## [73] labeling_0.4.2 htmlwidgets_1.5.4 tidyselect_1.1.2 ## [76] magrittr_2.0.3 bookdown_0.26 R6_2.5.1 ## [79] generics_0.1.2 multcomp_1.4-19 DBI_1.1.2 ## [82] mgcv_1.8-40 pillar_1.7.0 foreign_0.8-82 ## [85] withr_2.5.0 datawizard_0.4.1 abind_1.4-5 ## [88] nnet_7.3-17 tibble_3.1.7 performance_0.9.1 ## [91] modelr_0.1.8 crayon_1.5.1 uuid_1.1-0 ## [94] utf8_1.2.2 rmarkdown_2.14 officer_0.4.2 ## [97] jpeg_0.1-9 grid_4.2.1 data.table_1.14.2 ## [100] digest_0.6.29 xtable_1.8-4 tidyr_1.2.0 ## [103] munsell_0.5.0 bslib_0.3.1 Back to top References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
